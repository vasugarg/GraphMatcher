# Distributed Graph Analysis with Hadoop and AWS EMR

### Author: Vasu Garg
### Email: vasug20@gmail.com

## Introduction

This project focuses on creating a MapReduce program in Hadoop and deploying it to AWS Elastic MapReduce (EMR). The primary objective is to process large graphs using Hadoop's distributed computing capabilities and analyze the differences between original and perturbed graphs. The project involves the use of the Guava library to generate induced subgraphs and a Breadth-First Search (BFS) algorithm to create shards that keep adjacent nodes together. This approach ensures that edges are not lost during the subgraph generation.  

Video Link: https://youtu.be/m5IzIb1mHRo
The video explains deployment of hadoop application in AWS EMR Cluster and the project structure

### Environment
```
OS: Mac

IDE: IntelliJ IDEA 2022.2.3 (Ultimate Edition)

SCALA Version: 3.2.0

SBT Version: 1.8.3

Hadoop Version: 3.3.3
```
### Running the test file

Test Files can be found under the directory src/test

````
sbt clean compile test
````

### Running the project

1) Clone this repository

```
git clone https://github.com/vasugarg/GraphMatcher
```
2) cd to the Project
```
cd GraphMatcher
```

3) Open the project in intelliJ
```
https://www.jetbrains.com/help/idea/import-project-or-module-wizard.html#open-project
```
4) Open application.conf under GraphMatcher/GenericSimUtilities/src/main/resources/application.conf and make sure the relevant folders are created in the project. Below is the explanation of the configuration settings that needs to be set for the program to run.

##### Configuration Settings (application.conf)

- **`task3Name` and `task4Name`**
  - **Purpose:** Defines the names of specific MaprReduce tasks in the application.

- **`graphDirectory`**
  - **Purpose:** Specifies the directory which stores the .ngs and YAML files.

- **`originalGraphFileName`**
  - **Purpose:** Sets the filename for the original graph.

- **`perturbedGraphFileName`**
  - **Purpose:** Specifies the filename for the perturbed graph.

- **`shardsDirectory`**
  - **Purpose:** Defines the directory path for shard-related files, such as `nodeShards.txt` and `edgeShards.txt`.

- **`yamlFilePath`**
  - **Purpose:** Sets the path to a YAML file associated with the application.

- **`mapReduceOutputDir`**
  - **Purpose:** Specifies the directory for saving MapReduce job output.

- **`nodeShardsfileName` and `edgeShardsfileName`**
  - **Purpose:** Define filenames for node shard and edge shard data.

- **Paths and Directories:**
  - Note that certain settings specify directory paths relative to the project's root directory.
  - For example, if `graphDirectory` is set to `"outputs/"`, it refers to the `outputs` directory within the project's root directory.
  - Similarly, `shardsDirectory` is set to `"src/main/resources/input/"`, which refers to the `input` directory within the `src/main/resources` directory of the project.
  - These relative paths are resolved based on the project's classpath, providing flexibility and independence from specific absolute file paths.
  - The `outputs/` folder already contains sample graph generated by setting the number of states as 300 which can be used to run the program.
  - In order to run the program on different graphs, you can execute `LoadGraph.scala` to generate new graphs and making the relevant changes to application.conf post that.
  - For deploying in AWS, we need to use the configurations under `NGSimulator.ngsCompareS3` section.

4) Running project via SBT Run. Please make sure to delete the `src/main/resources/output` folder in project root before you run the below command.

```
sbt run
```

## Project Structure

The project comprises the following key components:

- **Induced Subgraphs**: Guava library is used to generate induced subgraphs, which are subsets of the original graph that preserve specific properties.

- **Shard Generation**: Shards are created by traversing the graph using a BFS algorithm. This method groups adjacent nodes together in the induced subgraphs, preventing edge loss.

- **MapReduce Jobs**: The project involves two MapReduce jobs: MapReduceEdges and MapReduceNodes.

    - **MapReduceEdges**: This job is responsible for processing edges in the graph. It calculates the similarity score between edges and generates combined similarity scores.

    - **MapReduceNodes**: This job processes nodes in the graph. It calculates the similarity score between nodes and computes the results for added, modified, removed, and matched nodes.

- **Similarity Score**: The project employs the Jaccard Index to calculate the similarity score between nodes and edges.

## Prerequisites

Before starting the project, ensure that you have the necessary tools and accounts set up:

1. **Hadoop**: Set up Hadoop on your local machine or cluster.

2. **AWS Account**: Create an AWS account and familiarize yourself with AWS EMR.

3. **Guava Library**: Ensure that you have the Guava library integrated into your project for induced subgraph generation.

4. **Java and Hadoop**: Make sure Java and Hadoop are installed and configured correctly.

5. **Git and GitHub**: Use Git for version control and host your project repository on GitHub.

6. **IDE**: Use an Integrated Development Environment (IDE) for coding and development.

## Usage

Follow these steps to execute the project:

1. **Data Preparation**: Ensure you have the original and perturbed graphs ready for processing.

2. **Configuration**: Set up the necessary configuration parameters and input files.

3. **Shard Generation**: Execute the shard generation process to prepare data for MapReduce jobs.

4. **MapReduce Execution**:

    a. Run the MapReduceEdges job to process edges and calculate similarity scores.
    
    b. Execute the MapReduceNodes job to analyze nodes and compute similarity scores.

5. **Results**: Examine the results obtained from the MapReduce jobs, which include similarity scores and details about added, modified, removed, and matched nodes and edges.

6. **Deployment on AWS EMR**: If required, deploy the project on AWS EMR for large-scale graph processing.

## Conclusion

This project demonstrates the power of Hadoop and MapReduce in processing and analyzing large graphs. By using induced subgraphs, BFS algorithms, and the Jaccard Index, it efficiently identifies similarities and differences between original and perturbed graphs. The project's structure and MapReduce jobs facilitate comprehensive graph analysis, making it a valuable tool for various real-world scenarios.

For detailed instructions on how to set up and run the project, please refer to the project's documentation and README files.

**Note:** This README provides an overview of the project. For detailed documentation and instructions, refer to the project's youtube video link and src files.
